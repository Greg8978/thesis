\ifdefined\included
\else
\documentclass[a4paper,11pt,twoside]{StyleThese}
\include{formatAndDefs}
\sloppy
\begin{document}
\setcounter{chapter}{1} %% Numéro du chapitre précédent ;)
\dominitoc
\faketableofcontents
\fi

\chapter{Prise de perspective et maintien de l'état mental des agents}
\label{chapter2}
\minitoc

\section{Motivation}
\label{sec:motivation}
Comme présenté dans le chapitre précédent, il est primordial que le robot comprènne son environnement pour pouvoir agir. Il doit avoir connaissance de l'état du monde en utilisant la perception qu'il en a à travers ses capteurs et utiliser cette perception pour en extraire, à l'aide de divers raisonnements, une estimation de la situation. Cependant, dans le cas où le robot agit en collaboration, ou tout du moins en présence d'humains, il est nécéssaire que le robot ne considère pas ces derniers comme de simples obstacles ou uniquement comme des systèmes agissant sur l'environnement. En effet, lorsque l'homme est présent, pour mesurer la qualité de l'interaction, il est important de considérer non seulement le succès et l'optimalité de l'execution, mais également la satisfaction et le confort des humains présents dans l'environnement ou impliqués dans la tâche. L'homme étant une créature sociale, le robot doit pouvoir faire preuve de capacité d'intérprétation de la situation de l'homme pour le comprendre et exhiber des comportements sociaux pour être compris et accepté par les individus avec lesquels il doit interagir.
Ainsi il est primordial que le robot comprènne son environnement et sa situation mais également celle des humains présent dans cet environnement.
Nous avons déjà présenté au chapitre précédent, comment certains composants du système d'évaluation de situation permettent d'obtenir des informations sur l'homme, notamment en \ref{sec:situationAgents}. Ici nous allons aller plus loin en considérant non seulement la situation spatiale de l'homme mais aussi sa situation mentale.

\section{Théorie de l'Esprit}
\subsection{Littérature Psychologique}
\label{sec:psy}

Afin de savoir comment le robot doit intéragir avec l'homme, il est important d'étudier tout d'abord comment les hommes interagissent entre eux.
La psychologie est la discipline étudiant les comportements humains.
L'un des domaines de cette discipline, appelé psychologie du développement, a pour but de comprendre comment et pourquoi l'humain se développe. Cela concerne aussi bien l'étude des processus mentaux, des comportements, des performances que l'évolution des habiletés au cours de la vie humaine.
La psychologie du développement s'attache à caractériser la théorie de l'esprit (Theory of mind en anglais, ou ToM). La théorie de l'esprit désigne la capacité qu'a un individu d'attribué un état d'esprit (en terme de pensées, ressentis, désirs, motivations et intentions) aux autres agents avec lesquels il interagit. La théorie de l'esprit inclut la notion de prise de perspective. Cette capacité humaine permets à un individu de raisonner en prenant le point de vue d'un autre.

Étudiée dans la littérature psychologique\cite{Flavell1992,Tversky1999}, cette habileté humaine est cruciale pour interagir avec autrui en permettant de raisonner sur ce que l'autre comprends du monde en terme de perception visuelle, de description spatiale, d'affordances et de croyances.
Des études menées sur des individus ne possédant pas les mécanismes cognitifs nécéssaires pour la prise de perspective, comme les jeunes enfants ou les personnes autistes \cite{frick2014picturing}, ont permis de mettre en évidence les difficultés que ces personnes ont dans leur relations sociales quotidiennes, ce qui confirme l'importance de cette capacité pour intéragir convenablement avec d'autres humains.

Flavell dans \cite{flavell1977development} décrit deux niveaux de prise de perspective. Il distingue: le niveau un étant la prise de perspective perceptuelle et le niveau deux, la prise de perspective conceptuelle.
La prise de perspective perceptuelle désigne la capacité d'un humain à comprendre que les autres ont une perception différente du monde, illustré par l'image \ref{fig:perceptuel}.

\begin{figure}[ht!]
 \centering
  \includegraphics[width=0.49\linewidth]{./img/perceptuel.jpg} 
  \caption {Illustration du fait que deux individus peuvent avoir une interprétation divergente de l'environnement en fonction de leur situation spatiale. La capacité de comprendre que l'autre a une vision différente est liée à la prise de perspective perceptuelle (ou prise de perspective de niveau un).}
  \label{fig:perceptuel}
\end{figure}

La prise de perspective conceptuelle va plus loin et désigne la capacité d'un humain à attribuer des croyances et des sentiments aux autres\cite{Baron1985}.
Ainsi, dans l'illustration faite dans la figure \ref{fig:conceptual}, Bob fait une supposition sur l'état de croyance d'Alice concernant la contenance de la boîte.
Cette supposition que Bob fait de l'état de croyance d'Alice peut différer de l'état réel.

\begin{figure}[ht!]
 \centering
  \includegraphics[width=0.49\linewidth]{./img/conceptual.jpg} 
  \caption {Illustration de la prise de perspective conceptuelle. Ici Bob attribue une croyance à Alice concernant la contenance de la boîte. Il suppose qu'elle croit que la boîte est vide. Cette faculté d'imputer des croyances aux autres s'appelle la prise de perspective conceptuelle (ou prise de prespective de niveau deux).}
  \label{fig:conceptual}
\end{figure}

Ainsi, dans la psychologie développementale, la faculté de comprendre qu'un autre puisse avoir une croyance érronnée sur l'environnement qui l'entoure est considérée comme une étape importante dans l'évolution et l'aquisition de la théorie de l'esprit. Dans la littérature psychologique, la tâche de reconnaissance de fausse croyance ("false belief task" en anglais) a été formulée dans \cite{Wimmer1983103}. Heinz Wimmer et Josef Perner définissent un test (le test de Sally et Anne) qui permets d'évaluer les aptitudes d'une personne à comprendre qu'autrui possède des états mentaux différents des siens. Le test a été par la suite réalisé par Simon Baron-Cohen et Alan M. Leslie et rapporté dans \cite{Baron1985} et une seconde version dans \cite{Leslie1988}. Ce test est expliqué dans la figure \ref{fig:sallyAndAnne}.

%TODO Put it in Annexe?
\begin{figure}[ht!]
 \centering
  \includegraphics[width=0.89\linewidth]{./img/sally.jpg} 
  \caption {Test de Sally et Anne permettant de vérifier la capacité d'un individu à identifier un état de connaissance érronné chez autrui.}
  \label{fig:sallyAndAnne}
\end{figure}


%Perspective Taking is a human ability which allows one to see things from other's point of view. 
% Studied in
% psychology literature~\cite{Flavell1992,Tversky1999}, this ability is
% crucial when interacting with people by allowing one to reason on
% others' understanding of the world in terms of visual perception, spatial
% descriptions, affordances and beliefs, etc.
% Therefore, in the last years it has been gradually
% employed in Human-Robot Interaction. ~\cite{breazeal2006} presents a learning algorithm that takes into account information about a
% teacher's visual perspective in order to learn a task. ~\cite{Johnson2005} apply visual perspective taking for action
% recognition between two robots.~\cite{Trafton2005} use both visual
% and spatial perspective taking for finding out the referent
% indicated by a human partner.

% In psychology, Theory of mind (ToM) is defined as an understanding of other people’s mental states (their thoughts, feelings, desires, motivations, intentions).
% It includes perspective taking ability. Visual perspective
% taking is one of the most significant ToM precursor. 
% ToM encompasses a wide range of skills from instantaneous visual
% perspective skill to complex interpretation of other agent intents, plans,
% feelings occurring on a long time period.
% Increased ToM skills directly lead to increased performance when interacting
% with other agent in a collaborative as well as a competitive context.
% Being able to attribute false belief (to recognize that someone else
% has different beliefs about the physical world) has been considered
% as a milestone in ToM development.
% In psychology literature the false belief task was formulated in
% \cite{Wimmer1983103}.
% Breazal in ~\cite{BreazealGB09} proposed one of the first human robot
% implementation and proposed some more advanced goal recognition skills
% relying on this false belief detection.


% This paper will present an evolution of the previous work \cite{Warnier2012a} \cite{Lemaignan2012} \cite{Sisbot2011} with a refined management of divergent beliefs, temporal reasonning on data and improved inferring capabilities.
% These improvements allow the robot to pass Sally and Anne test \cite{Baron1985} (see Section IV B), to make inferences not only on position properties but also on dialogue, and other human actions.

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% ROMAN TOASTER 2016

\subsection{Usage en Robotique}

Comme indiqué dans la partie \ref{sec:motivation}, pouvoir percevoir et raisonner sur l'environnement qui l'entoure sont des capacités nécéssaires pour le robot mais non suffisantes lorsqu'il intéragit avec des humains. Pour pouvoir comprendre la situation de l'humain, des recherches récentes ont tenté d'implémenter une sorte de théorie de l'esprit, en permettant au robot d'avoir la faculté de prise de perspective de niveau un (perceptuelle). Cynthia Breazeal présente dans \cite{breazeal2006} un algorithme d'apprentissage qui prends en compte l'information concernant le point de vue visuel de l'instructeur afin d'apprendre convenablement une tâche. Dans \cite{Johnson2005}, les auteurs appliquent la prise de perspective visuelle pour la reconnaissance d'action entre deux systèmes robotiques. Gregory Trafton dans \cite{Trafton2005} utilise à la fois la prise de perspective visuelle et spatiale pour identifier le référent indiqué par un partenaire humain. Dans \cite{ros2010one} la prise de perspective visuelle est utilisée pour simplifier la clarification de déclarations référentielles (referential utterances) dans des scénarios impliquant plusieurs objets.

Pour pleinnement comprendre et se faire comprendre par l'homme, certaines études ont également visé à donner le niveau deux (conceptuel) de prise de perspective au robot. Cynthia Breazal dans ~\cite{BreazealGB09} propose l'une des première implémentation faisant intervenir un humain et un robot ainsi que quelques capacités plus avancées de reconnaissance de but basé sur cette détection de fausse croyance.
%TODO Compléter la biblio sur le niveau 2
%use: http://www.csc.kth.se/~hedvig/publications/cogsci_16.pdf

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% This paper will present an evolution of the previous work \cite{Warnier2012a} \cite{Lemaignan2012} \cite{Sisbot2011} with a refined management of divergent beliefs, temporal reasonning on data and improved inferring capabilities.
% These improvements allow the robot to pass Sally and Anne test \cite{Baron1985} (see Section IV B), to make inferences not only on position properties but also on dialogue, and other human actions.

%ROMAN2014
% Secondly, it must be able to gain explicit reasoning on the human it
% interacts with. It means that not only the knowledge must be
% grounded between the robot and its interactor but also that the robot
% must be able to maintain an explicit representation of the knowledge
% of its interactor apart of its own knowledge. That will allow the
% robot to compare its own beliefs with the one of the human and to
% infer similarities as well as differences and ambiguities. Thus, the
% robot must be able to handle a kind of "theory of mind" \footnote{\url{http://en.wikipedia.org/wiki/Theory\_of\_mind}}.


% Equipped with such capabilities, a robot who will interact
% with humans should be able to extract, compute or infer these
% relations and capabilities in order to communicate and interact efficiently in a natural way.

% To achieve this we identified 3 main ingredients.
% First, the consideration of perspective-taking, i.e. the ability of
% the robot not only to build a model of the world for itself but
% also to estimate what its human partners perceive.
% Secondly, the ability to compute efficiently affordances for itself
% and to estimate the affordances of its human partners in a given situation 
% Finally, the ability to
% maintain a history of beliefs based on presence and focus of attention of humans which will enable reasoning on divergent beliefs.
% We will present how these features are implemented in SPARK as a permanent activity based on inter-related processes.


%%%%%%%%%%%%%%%%%%%%%%%%
% TOASTER ROMAN2016

% To make robots more socially competent, some research aims to endow robots with this ability.
% Some robotic research use the first level to have a better understanding of the human and remove ambiguities ~\cite{Trafton2005}, ~\cite{breazeal2006}, ~\cite{ros2010one}
% %B conceptual
% Concerning level two, various research on human robot interaction already aim to represent the human belief state.
% Breazal et al.~\cite{BreazealGB09} proposed one of the first human-robot implementation. In our previous work \cite{Milliez2014}, we made a primitive implementation to solve the Sally and Anne test described by Wimmer in~\cite{Wimmer1983}. In this primitive implementation, the reasoning on others belief state was limited to object position. We propose here a more generic approach to represent any kind of belief the human may hold on the environment.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Prise de Perspective et État Mental}

\subsection{Prise de Perspective Perceptuelle}
\label{sec:perceptuelle}

Dans notre système, pour donner au robot la capacité de se mettre à la place de l'homme et de comprendre ce qu'il est capable de percevoir et les éléments qui sont à sa porté, nous avons ajouté le calcul de deux faits basés sur la représentation tridimensionnelle de l'état du monde expliquée au chapitre précédent. Le premier permet de connaître ce qui est visible par l'homme, c'est à dire ce qui se trouve dans son champs visuel et n'est pas caché (ou tout du moins qui est suffisemment visible).

% To estimate what is visible for a human, it computes which objects are present in a cone, emerging from human's head. 
% If the object can be directly linked to the human's head with no obstacle and if it is in the field of the view cone, 
% then we assume that the human sees the object and hence has knowledge of its position. 
% If an obstacle is occluding the object, then it won't be visible for the human. 
% Concerning the reachability, a threshold of 1 meter is used to determine if the human can reach an object or not.


Ainsi, dans la figure \ref{fig:occludedHuman}, l'humain en bleu est incapable de voir l'objet à droite (WHITE\_BOOK) car il est caché par la boîte grise (GREY\_BOX).


%TODO: surligné l'objet occlut
\begin{figure}[ht!]
 \centering
  \includegraphics[width=0.99\linewidth]{./img/blueMovedPr2M.png} 
  \caption {Exemple d'état du monde où le système est capable de calculer la visibilité des objets pour chaque agent. Ici, le livre blanc n'est pas visible par l'homme.}
  \label{fig:occludedHuman}
\end{figure}


Le robot est donc capable de savoir ce qu'il est capable de percevoir (moyennant éventuellement un mouvement de tête), mais également ce que l'homme peut ou non percevoir. Nous vérons dans le chapitre suivant comment cela peut être utilisé durant une intéraction, notamment pour améliorer le dialogue situé en terme d'éfficacité.

Le second fait ajouté concerne le calcul permettant de savoir si les objets qui entourent l'homme sont à sa porté. En utilisant la bibliothèque 3D Move3D  \cite{Simeon2001}, il est en effet possible de calculer la cinématique d'un agent et de planifier ses mouvements. Grâce à cela, il est donc possible de savoir si un objet est accécible à cet agent. Ce calcule est illustré par l'image \ref{fig:reach}, où le robot calcul l'accéssibilité d'un objet par rapport à lui même et à l'homme présent dans la scène.



\begin{figure}[ht!]
 \centering
  \includegraphics[width=0.99\linewidth]{./img/reach.jpg} 
  \caption {Exemple d'état du monde où le système calcul si l'homme est capable d'atteindre un objet. Sur l'image de gauche, le robot est capable de calculer qu'il peut attrapper l'objet gris. Il peut également calculer que l'homme peut attraper l'objet gris, comme illustré par l'image centrale. Si l'homme avait été placé plus loin, il aurait été incapable de l'attrapé, comme illustré par l'image de droite.}
  \label{fig:reach}
\end{figure}


La liste des faits (simplifiés) générés pour cette scéne concernant l'accécibilité de l'objet gris serait pour les différentes configurations:

\begin{scriptsize}
\begin{verbatim}
left and center pictures                   right picture

Subject   property  target    value        Subject   property  target    value

ROBOT     canReach  GREY_OBJ  true         ROBOT     canReach  GREY_OBJ  true
HUMAN     canReach  GREY_OBJ  true         HUMAN     canReach  GREY_OBJ  false
\end{verbatim}
%\end{small}
%\end{footnotesize}
\end{scriptsize}

Pouvoir connaître quels objets sont à la porté de l'homme permets au robot de générer des plans collaboratifs prenant en compte cette donnée pour l'allocation de tâche.
%TODO cite paper
Par exemple, si la tâche collaborative est de ranger des livres dans une boîte, si on a les faits:

\begin{scriptsize}
\begin{verbatim}

                                   Subject   property  target     value

                                   ROBOT     canReach  BLUE_BOOK   true
                                   ROBOT     canReach  WHITE_BOOK  false
                                   ROBOT     canReach  BASKET      false
                                   HUMAN     canReach  BLUE_BOOK   false
                                   HUMAN     canReach  WHITE_BOOK  true
                                   HUMAN     canReach  BASKET      true
\end{verbatim}
%\end{small}
%\end{footnotesize}
\end{scriptsize}

le plan généré prendra en compte que le livre bleu (BLUE\_BOOK) et le panier (BASKET) sont atteignables par le robot pour lui assigner la tâche de ranger ce livre. De même, comme le livre blanc (WHITE\_BOOK) et le panier sont atteignables par l'homme, la tâche de le ranger lui sera assignée.
Ce fait peut également aider à la désambiguation, par exemple si l'homme demande au robot d'amener un objet, il est peu probable que l'homme parle d'un objet qui soit à sa porté.


%TODO conclude on perspective taking with those two facts: helps to compute collaborative plan and blabla
%TODO introduce the following

%mais également comprendre la situation de l'homme avec lequel il intéragit.
Pour comprendre l'homme, il est également important de comprendre sa situation spatiale, à savoir comment celui-ci est entouré et comment il est suceptible de décrire ce qui l'entour par rapport à sa position.

%TODO spatial situation
%Perspective taking?
To improve this component from our previous work, we added requests concerning the relative position of entities toward an agent's point of view. When requested, it can tell on which side is an entity (left, right, back, front). It can also tell the relative position of an entity, compared to another, from the agent's perspective. For example, it makes possible for the robot to tell to the human "the mug you are looking for is \textbf{on your right}" or "please give me the book which is \textbf{for you on the left of the red mug}".
This kind of requests enhance the robot with a geometrical perspective taking ability to talk to the human with his own references, improving the social aspect of the system.

\begin{figure}[ht!]
 \centering
  \includegraphics[width=0.59\linewidth]{./img/relative_position.jpg} 
  \caption {}
  \label{fig:relativePose}
\end{figure}


\subsection{Prise de Perspective Conceptuelle}
\label{sec:beliefm}
%\subsubsection{Suivi des États Mentaux}
Nous avons expliqué dans le chapitre 2 qu'il est important de faire des hypothèses sur la position des objets pour pouvoir en assurer le suivi. Bien que nécéssaire, le fait de créer des hypothèse implique la possibilité de se tromper.
En conclusion du chapitre 2, nous avons expliqué comment il est possible de probabiliser l'état du monde pour prendre en compte l'incertitude du robot.
Dans cette section nous allons expliquer comment la prise de perspective va permettre au robot d'avoir connaissance de l'état croyances des hommes avec lesquels il intéragit et ainsi savoir lorsqu'ils ont une croyance erronnée concernant l'environnement.

La première étape pour pouvoir connaître l'état mental d'un humain est d'avoir connaissance des informations qu'il reçoit. L'humain peut recevoir des informations sur son environnement de deux façons: soit il les reçoit dirèctement (ou les déduit) de ses capacités perceptive, soit d'un autre agent à travers le dialogue.
Pour donner au robot la capacité de prise de perspective décrite en \ref{sec:psy}, nous nous focalisons dans un premier temps sur la perception et sur le dialogue homme-robot pour déduire quelles informations l'humain peut aquérir.

Grâce aux calculs des affordances des agents expliqués en \ref{sec:perceptuelle}, il est possible de savoir lorsqu'un agent perçoit un objet. Lorsqu'un humain voit un objet, cela implique qu'il aquiert la connaissance de sa localistion, et donc mets à jour cette information dans son état d'esprit. Cela permets donc de suivre l'état de connaissance de chaque agent.

L'état mental de chaque agent (tel qu'il est estimé par le robot) est ainsi concervé et mis à jour dans un modèle séparé de l'état de connaissance que le robot lui-même possède sur l'environnement. Ainsi, le modèle représentant l'état mental de chaque agent se traduit par une liste de faits. Chaque modèle est indépendant et cohérent d'un point de vu logique.

% In SPARK we have the position of humans (see III 1.). We use it to
% calculate affordances of each human toward elements of the scene he can interact with.
% To estimate what is visible for a human, it computes which objects are present in a cone, emerging from human's head. 
% If the object can be directly linked to the human's head with no obstacle and if it is in the field of the view cone, 
% then we assume that the human sees the object and hence has knowledge of its position. 
% If an obstacle is occluding the object, then it won't be visible for the human. 
% Concerning the reachability, a threshold of 1 meter is used to determine if the human can reach an object or not.

%comment peut etre mettre un exemple de modèle d'agent,
%  même tout petit pour mettre les idées en place ?}


%\subsubsection{Géstion de Croyance Divergente}
Dans certains cas, l'homme et le robot peuvent avoir un modèle qui contient des valeurs différentes. Cela peut provenir d'une vision différente de la scène (e.g. certaines propriétés sont accessibles uniquement au robot, donc l'homme n'en a pas connaissance).
Cela peut provenir de croyances divergentes entre l'homme et le robot (e.g. l'homme croit qu'un objet a la propriété P alors que le robot sait que cette propriété est fausse). Dans ce second cas, la prise de perspective n'est pas suffisante pour comprendre la croyance erronnée de l'homme. Il est nécéssaire de mettre en place la gestion de croyance divergente (divergent belief management en Anglais).


% This Management relies on data from the environment as well as from
% affordances and supervisor.
% This way, the robot can generate beliefs according to the task stated in correlation with  affordances 
% as shown in fig. ~\ref{beliefs_fg}.

% \begin{figure}[ht!]
%  \centering
%   \includegraphics[width=0.99\linewidth]{./img/beliefs2.png} 
%   \caption {Schema of SPARK reasoning to generate beliefs}
%   \label{beliefs_fg}
% \end{figure}
Par exemple, si on considère qu'un humain appellé \textit{GREG} quitte la zone d'intéraction avec la croyance qu'un objet \textit{Obj} a la propriété \textit{Prop} dans un état \textit{e1}. 

On présente ci-dessous l'état de connaissance du robot et celui de \textit{GREG} tel qu'il est modélisé par le robot.

\begin{scriptsize}
\begin{verbatim}
          ROBOT                  GREG
      Obj Prop e1            Obj Prop e1
\end{verbatim}
%\end{small}
%\end{footnotesize}
\end{scriptsize}

Durant son absence, imaginons que la propriété \textit{Prop} appliquée à \textit{Obj} a évolué pour atteindre un état \textit{e2}.
Lorsque \textit{GREG} revient, même si la propriété a évolué, il pensera probablement que la propriété est dans le même état que lorsqu'il a quitté la scène, jusqu'à ce qu'il soit capable de réévaluer ses croyances en utilisant le dialogue ou la perception. Pour prendre en compte ce type de situation, le robot garde inchangé le modèle d'un agent lorsque celui-ci n'est pas présent pour observer les actions ou constater les changements induits, et ce jusqu'à son retour.

Avant que l'homme ne revienne, la croyance du robot a donc évoluée mais pas celle de l'homme.

\begin{scriptsize}
\begin{verbatim}
          ROBOT                  GREG
      Obj Prop e2            Obj Prop e1
\end{verbatim}
%\end{small}
%\end{footnotesize}
\end{scriptsize}

Lorsque l'homme revient, on identifit trois situations différentes:

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%illustrate each situation with images
\begin{itemize}
\item Dans le premier cas, l'homme remarque le changement d'état de la propriété et corrige directement sa croyance erronnée. Le robot corrige donc automatiquement le vecteur de faits représentant les croyances de \textit{GREG} avec le nouvel état de la propriété. 

%illustrate each situation with images

\begin{scriptsize}
\begin{verbatim}
          ROBOT                  GREG
      Obj Prop e2            Obj Prop e2
\end{verbatim}
%\end{small}
%\end{footnotesize}
\end{scriptsize}


Pour illustrer, on peut prendre comme objet une radio (\textit{RADIO}) et comme propriété l'état (\textit{turnedOn}) qui peut avoir comme valeur allumée (\textit{TRUE}) ou éteinte (\textit{FALSE}). Si \textit{GREG} quite la scène en sachant que la radio est allumée, lorsqu'il revient, n'entendant plus la radio, il pourra dirèctement déduire la nouvelle valeur de la propriété d'état liée à la radio (éteinte).

%illustrate each situation with images

\begin{scriptsize}
\begin{verbatim}
          ROBOT                  GREG
    RADIO turnedOn FALSE   RADIO turnedOn FALSE
\end{verbatim}
%\end{small}
%\end{footnotesize}
\end{scriptsize}


\item La deuxième situation, l'homme remarque le changement mais est dans l'incapacité de connaître le nouvel état de la propriété. L'humain étant au courant de son ignorance, le robot supprime sa croyance erronnée. Pour ce faire, il mets à jour le vecteur de faits avec un état \textit{unknown} qui traduit le fait que le robot ait connaissance que l'homme sait qu'il ignore le nouvel état de la propriété.


\begin{scriptsize}
\begin{verbatim}
          ROBOT                  GREG
      Obj Prop e2            Obj Prop unknown
\end{verbatim}
%\end{small}
%\end{footnotesize}
\end{scriptsize}



On peut prendre cette fois comme exemple un livre (\textit{BOOK}) ayant comme propriété une position (\textit{isOn}) et pour valeur le meuble sur lequel il se trouve (\textit{LIVINGROOM\_TABLE}, \textit{BEDSIDE\_TABLE}). Si \textit{GREG} quite la scène en sachant que le livre est sur la table du salon, lorsqu'il revient, s'il observe que le livre n'est plus à sa place, il aura alors connaissance que la propriété de position a changé de valeur sans être capable de savoir la nouvelle valeur de celle-ci tant qu'il n'aura pas vu l'objet.

\begin{scriptsize}
\begin{verbatim}
          ROBOT                         GREG
     BOOK isOn LIVINGROOM_TABLE    BOOK isOn unknown
\end{verbatim}
%\end{small}
%\end{footnotesize}
\end{scriptsize}



\item Dans la troisième situation, l'homme est incapable de remarquer que la propriété a changée. L'humain va alors garder sa croyance erronnée concernant la propriété jusqu'à ce qu'il puisse constater un changement ou que le robot l'informe de l'évolution de la propriété. 

\begin{scriptsize}
\begin{verbatim}
          ROBOT                  GREG
      Obj Prop e2            Obj Prop e1
\end{verbatim}
%\end{small}
%\end{footnotesize}
\end{scriptsize}

Pour prendre un dernier exemple, si l'objet est une boîte de cookies (\textit{COOKIE\_BOX}) opaque et que la propriété décrit la présence de cookies à l'intérieur de la boîte (\textit{isFull}), pouvant prendre pour valeurs pleine (\textit{TRUE}) ou vide (\textit{FALSE}). Si lorsque \textit{GREG} quite la scène il pense que la boîte est vide, si celle-ci a été remplit pendant son absence, \textit{GREG} sera incapable de savoir que la boîte est apprésent remplie tant que celle-ci sera fermée.

\begin{scriptsize}
\begin{verbatim}
           ROBOT                         GREG
COOKIE_BOX isFULL TRUE        COOKIE_BOX isFULL FALSE
\end{verbatim}
%\end{small}
%\end{footnotesize}
\end{scriptsize}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5





Knowing these beliefs is a great help to the robot to understand human and interact with him.
The robot takes human's perception into account to have appropriate interpretation of human requests, to proactively inform or warn the human about a missing or wrong belief and also to generate a collaborative plan. 
Thus, the robot has to reason on what a human can see, reach and
knows to get these skills.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
ROMAN2016 INTENTION
We consider our world as a dynamic environment, where entities can move, objects' properties can change and actions can be performed.  We define an action as a tuple $(name, preconditions, target, postconditions)$. The $name$ of an action is a unique string that identifies it. The $preconditions$ are a list of properties that must be true in order to realize the action. In our system, an action is executed on a $target$, which can be a physical object, like a cup, but also an area of the environment, like a room. The $postconditions$ are the set of properties, and their values, affected by the action's execution.

Since we are interested, in this paper, on reasoning and not perceptual aspects, we use inference, as explained in Sec. \ref{sec:intention_recognition}, in order to understand when a human has performed an action. Through the predefined $postconditions$ of actions we can also infer changes in object properties, e.g. the human opens a box, so the box is now open. 

Since the environment is dynamic, agents can have divergent representations of the world. To model this aspect, the information produced by perception, geometrical reasoning and inference, are collected by the robot in \textit{belief models}, built for itself and for each agent. A \textit{belief model} is a symbolic representation of the world state, as known by an agent. In a model, the world state is represented by properties and values. To represent the lack of knowledge of an agent, the value of a property can be \textit{unknown}.
We created a rule based framework in order to build beliefs of each agent and update them when needed. Human belief models are updated using the perspective taking skills of the robot. When the robot detects the execution of an action in the world, it updates the belief model for itself and for every human that can perceive the action, adding the action's $postconditions$ to their models. When an action is not perceived by a human (e.g. the user was in an other room), his belief model won't be updated, as he is not aware of the changes that occurred.

However, when he comes back and looks at the environment, we assign him a new belief state following a set of rules, which we will now explain. We call $H$ the agent, $HB$ his belief model, and $RB$ the robot's belief model. We also create the following predicates: $obs(p)$ means that the instance of property $p$ is observable, $valid(p,x)$ means that the instance of property $p$ doesn't contradict the current perception data of agent $x$, $value(p,m)$ is the value of predicate $p$ in belief model $m$, and $vis(p,x)$ means that agent $x$ has visibility on the linked entities of property $p$. The rules for the $valid$ predicate will be different in each property. For example the property \textit{MUG isOn TABLE} won't be valid for agent Max if he can see that there is no mug on the table. For each property $p\in HB \cup RB$:
\begin{itemize}
\item if $p \in RB, \quad p\not\in HB,\quad obs(p),\quad vis(p,H) \rightarrow value(p,HB)=value(p,RB)$.
\item if $p \not \in RB,\quad p\in HB,\quad obs(p),\quad vis(p,H) \rightarrow remove\quad $p$ \quad from \quad HB$.
\item if $p\in RB,\quad p\in HB$ then:
	\begin{itemize}
      \item if $value(p,HB)\neq value(p,RB),\\ \quad obs(p),\quad vis(p,H) \rightarrow \\ value(p,HB)=value(p,RB)$.
      \item if $value(p,HB)\neq value(p,RB),\\ \quad !obs(p),\quad !valid(p,H) \rightarrow \\ value(p,HB)=\textit{unknown}$.
	\end{itemize}
\end{itemize}
The idea of this set of rules is updating an agent's mental belief model for a property only if it's observable, or if it's not observable and perception data contradicts the current value of the property (e.g. the mug was moved from the table to the kitchen while the agent was in another room: while the agent can not see where is the mug, he can see it is no longer on the table).



\section{Implémentation}
%BASE DE DONNÉE
%(Implémentation)
%we assumed in previous chapter that perception object update property.
%However, for some properties visual, some different...
We assume here that if the human is able to perceive an object property, he is then aware of the object property state. As an example for position property, if there is no obstacle between his eyes and the object and the object is in the human field of view, 
he is then aware of its position.

%explain how we do -> observability
%explain that it is a first approximation, hypothesis are based on lot of things.
%confidence should shrink with time
%Database, cf roman 2016

%TODO: then detail how to update mental states: different for position hypothesis (sub symbolic) and other properties. observability Or maybe put it in implementation?
%Visual pereception -> position, light on...
%Some are not easily perceived by vision: hot, isFull...

\section{Résultats Expérimentaux}
%Sally and Anne + experience améliorée, cf Roman 2014

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Nous présentons ici l'une des expériences menée au LAAS permettant de mettre en valeur la capicité du système à maintenir et mettre à jour l'état mental des humains présents.

\subsection{Configuration Expérimentale}
Tout d'abord, nous présentons brièvement les capteurs et modules que nous utilisons pour les expérimentations.


Ces expérimentations ayant été réalisées en 2014, l'infrastructure logicielle utilisée pour l'estimation de la situation est SPARK, qui est une version antérieure à TOASTER mais qui a un fonctionnement similaire pour le suivi de l'état mental des humains concernant les propriétés de position des objets.

%TODO: redo
\begin{figure}[ht!]
  \centering
  %% \includegraphics[width=0.7\textwidth]{./figs/supervisor-architecture-new.pdf} \\
% \includegraphics[width=0.5\textwidth]{./image/sparkinput.png} \\
 \includegraphics[width=0.99\linewidth]{./img/spark_input.png} 
  \caption {Schema of SPARK input implementation}
  \label{input_fg}
\end{figure}

Pour assurer le suivi des humains, nous utilisons une Asus xtion. Comme le fait de bouger la tête une partie importante de la communication pour permettre au robot de montrer son attention, et pour éviter la perte du suivi, nous avons fixé l'Asus xtion sur une base stable derrière le robot.
Un module, appelé niut, est responsable de la gestion du suivi des humains, en utilisant l'API OpenNI. Ce module détecte également la teinte moyenne du haut des humains afin de les identifier. Pour obtenir la valeur moyenne de la teinte de chaque humain, on extrait la position du torse, puis nous la projetons dans les coordonnées de l'image RGB. Si les données sont disponibles, le module utilise également les coordonnées des épaules et des hanches pour définir le rectangle dans lequel la teinte sera calculée, sinon le module créé un rectangle au tour de la position du torse, en adaptant l'échelle en fonction de l'éloignement de l'humain.
En donnant au module les valeurs des teintes de chaque humains, il est possible de reconnaître l'humain et d'envoyer ses données de postures au module d'estimation de la situation avec un id unique. Cela permets de filtrer les faux positifs dans la détection d'humains et évite que des humains non enregistrés perturbent le déroulement des expériences.
Pour que les hommes soient à la bonne position dans le modèle du monde, une projection est effectuée à partir de la position et l'orientation de l'Asus xtion.




La posture et la position du robot sont dirèctement obtenues grâce à ses capteurs internes en utilisant l'intergiciel ROS.

Enfin, pour obtenir la position des objets, nous utilisons le module Viman. Ce module utilise la vision stéréo du robot pour reconnaître et localiser les objets. Pour ce faire, viman scan des RTags sur les objets.
%TODO ref to rtags
Une fois que la position relative (à la caméra) de l'objet est connue, elle est envoyée au module d'estimation de la situation qui, en utilisant la position de la tête du robot pourra obtenir la position de l'objet dans le monde par projection.
La figure \ref{input_fg} illustre cette implémentation.







\subsection{Résultats}


Nous avons introduit ci-dessus comment nous procédons pour suivre les croyances distinctes pour chaque agent. Nous croyons que cette fonctionnalité est utile pour comprendre la verbalisation de l'homme, ses actions et la focalisation de son attention, i.e. pour interagir avec des humains. Comme le robot connait les croyances des humains il peut décider de quelle information est-il nécéssaire de fournir à l'homme et également si il doit parler ou non en fonction de la situation actuelle ou de l'état de réalisation du plan collaboratif. La fonctionnalité de gestion de croyance présenté dans ce chapitre a été utilisée sur un robot afin de les tester \footnote{Des vidéos des expérimentations sont accessibles à l'url: \url{http://homepages.laas.fr/gmilliez/roman2014/}}.




\subsubsection{Le Test de Sally et Anne}

Pour aider à la compréhension des scénarios, la figure \ref{SA} et la figure  \ref{divB} sont des images de l'expérimentation réelle avec des captures d'écran de la modélisation 3D du module de raisonnement spatial.

Dans le premier scénario, afin de tester notre système et d'illustrer l'habileté du robot à détecter les \emph{croyances fausses/divergentes sur la position d'un objet} nous avons décider de le soumettre au test de Sally et Anne. Dans notre expérience, deux utilisateurs Greg (en vert) et Bob (en bleu) font face au robot
La scène est composé d'un libre blanc et de deux boîtes se trouvant sur une table \ref{initSA}).
Greg mets le livre blanc sous la boîte rose (\ref{startSA}). Puis Greg quite la scène. Pendant que Greg est parti, Bob prends le livre blanc et le mets sous la petite boîte. (\ref{middleSA} et \ref{endSA}).
Puis, quand Greg revient, nous demandons au robot où Greg pense que le livre se trouve.


\begin{figure*}[ht!]
  \begin{center}
    \subfigure[Greg (en vert) et Bob (en bleu) font face au robot. Ils connaissent la position de chaque objet.]{\includegraphics[width=0.49\textwidth]{./SallyAndAnne/1mas.png}\label{initSA}}
    \subfigure[Greg mets la boîte rose sur le livre blanc.]{\includegraphics[width=0.49\textwidth]{./SallyAndAnne/2mas.png}\label{startSA}}
    \subfigure[Greg pars et Bob enlève le livre blanc de la boîte rose.]{\includegraphics[width=0.49\textwidth]{./SallyAndAnne/3mas.png}\label{middleSA}}
    \subfigure[Bob mets le livre blanc sous la petite boîte, puis Greg revient. Le robot est capable de comprendre que Greg croit que le livre est sous la boîte rose.]{\includegraphics[width=0.49\textwidth]{./SallyAndAnne/4mas.png}\label{endSA}}
 \end{center}
  \caption{Le test de Sally et Anne dans l'environnement réel (partie de gauche) et tel qu'il est perçu par le robot (partie de droite). Pour aider à la compréhension, nous montrons explicitement les croyances concernant la position du livre blanc. Les croyances de Greg sont indiquées avec des flèches vertes et celles du robot avec des flèches rouges.
Les petites sphères de couleur dans l'environnement perçu sont utilisées comme des indications sur l'accésibilité d'un objet pour un agent.}
  \label{SA}
\end{figure*}



Nous allons présenter ci-dessous quelques faits symboliques interessants pour Greg et le robot correspondant à la situation décrite en \ref{endSA}.
%TODO update facts
\begin{scriptsize}
\begin{verbatim}
          ROBOT                         GREG
GREG canSee PINK_BOX  TRUE      GREG canSee PINK_BOX  TRUE
GREG canSee SMALL_BOX TRUE      GREG canSee SMALL_BOX TRUE
GREG canSee BOOK      FALSE     GREG canSee BOOK      FALSE
BOOK isIn   SMALL_BOX           BOOK isIn   PINK_BOX

\end{verbatim}
\end{scriptsize}

Comme le robot sait que le livre n'est pas visible par Greg, il n'a pas mis à jour ses coryances concernant la position de cet objet. Avec l'aide du système d'estimation de la situation maintenant le modèle de croyance des agents, le robot est capable d'observer que, comme Greg ne peut observer ce qui a changé, il a une croyance divergente sur la position de l'objet. Si l'homme et le robot doivent accomplir une tâche impliquant cet objet, le robot sera capable de savoir qu'il doit informer l'homme de la position de l'objet.

\subsubsection{Gestion de Croyance Divergente}


Pour aller plus loin et illustrer les trois différentes situations présentées en \ref{sec:beliefm}, nous avons tourné un autre scénario. Dans cette deuxième expérimentation, deux utilisateurs, Greg et Bob, font face au robot. Ils ont un livre blanc et une boîte blanche qui servira pour faire de l'occlusion visuelle (\ref{initDB}).
Une fois que Bob pars (\ref{startDB}) Greg prends le livre blanc et le mets derrière la boîte blanche pour qu'il ne soit pas visible du point de vue humain (\ref{startDB}).
Puis Greg pars et Bob revient. Le robot est capable de comprendre que, comme le livre blanc est caché, Bob ne sait pas où il se trouve. Toutefois, Bob peut voir que le livre n'est plus là où il pensait qu'il serait. Cette information manquante est représenté par une sphère transparente à l'emplacement où Bob pensait trouver l'objet (\ref{lackBobDB}).


\begin{figure*}[ht!]
  \begin{center}
    \subfigure[Greg (en vert) et Bob (en bleu) font face au robot. Ils connaissent la position de chaque objet.]{\includegraphics[width=0.49\textwidth]{./DivBFull/1ms.png}\label{initDB}}
    \subfigure[Bob pars, Greg mets le livre blanc derrière la boîte blanche.]{\includegraphics[width=0.49\textwidth]{./DivBFull/2ms.png}\label{startDB}}
    \subfigure[Greg pars et Bob revient. Bob ignore la position du livre blanc. Le robot est au courant de ce manque d'information (une sphère bleu transparente représente la dernière croyance de Bob concernant la position du livre blanc).]{\includegraphics[width=0.49\textwidth]{./DivBFull/3ms.png}\label{lackBobDB}}
    \subfigure[Bob regarde derrière la boîte blanche (et donc mets à jour ses croyances) et prends le livre blanc. ]{\includegraphics[width=0.49\textwidth]{./DivBFull/4ms.png}\label{updateBobDB}}
    \subfigure[Bob pars avec le livre blanc, puis Greg revient. Greg pense toujours que le livre blanc est derrière la boîte blanche. Le robot est au courant de cette croyance divergente (Une sphère opaque verte représente la croyance actuelle de Greg concernant la position du livre blanc).]{\includegraphics[width=0.49\textwidth]{./DivBFull/6ms.png}\label{GregDB}}
    \subfigure[Greg regarde derrière la boîte blanche et est alors informé que sa croyance est erronée mais ne connaît pas la nouvelle position de l'objet (la sphère devient transparente).]{\includegraphics[width=0.49\textwidth]{./DivBFull/7ms.png}\label{lackGregDB}}
 \end{center}
  \caption{Scénario de croyance divergente. Dans l'environnement réel (partie de gauche) et tel qu'il est perçu par le robot (partie de droite).}
  \label{divB}
\end{figure*}

Ci-dessous sont représentés quelques faits symboliques concernant les croyances de Bob et du robot tel que le robot les modèlise.

\begin{scriptsize}
\begin{verbatim}
          ROBOT                                    BOB
ROBOT  canSee   WHITE_BOX TRUE          ROBOT  canSee WHITE_BOX   TRUE    
BOB    canSee   WHITE_BOX TRUE          BOB    canSee   WHITE_BOX TRUE
BOB    canSee   BOOK      FALSE         BOB    canSee   BOOK      FALSE
BOOK   isNextTo WHITE_BOX TRUE          BOOK   location           unknown
\end{verbatim}
\end{scriptsize}

A l'étape suivante, Bob regarde derrière la boîte blanche et mets à jour ses croyances (\ref{updateBobDB}).
Puis il prends le livre blanc et s'en va. Lorsque Greg revient, comme l'objet était caché auparavant et est toujours pas visible, le robot est capable de comprendre que Greg n'a pas mis à jour ses croyance et a donc une croyance erronée sur la position du livre blanc. Cette croyance erronée est représentée par une sphère opaque à l'emplacement où Greg pense que le livre se trouve (\ref{GregDB}).
Puis Greg regarde derrière la boîte blanche. Dès qu'il voit que le livre n'y est pas, il n'a plus de croyance divergente mais il ignore toujours où se trouve le livre. Comme le robot a suivi cette action d'observation, il a mis à jour le modèle d'état mental de Greg. La sphère représentant la croyance de Greg sur la position du livre est alors devenue transparente (\ref{lackGregDB}).

Sans notre algorithme, à chaque étape le robot penserait à chaque étape que les humains ont la même croyance que lui concernant la position des objets (à savoir que l'humain connait la nouvelle position de l'objet).

%REDIT avec chapter4
% \subsubsection{Dialog disambiguation}


% Now we will show how dialog could benefits from
%   our system. 
% At the end of the scenario of fig  ~\ref{divB}, Bob left with the white book. The robot was able to see this action by using the monitoring spheres. Now, let's assume that in
% addition to this setup, a black book stands on the table but is hidden by the pink box on Bob's side. So the black book is not visible by
% Bob and is visible by the robot.
% Consequently, if Bob asks the robot "where is the book?", as the robot knows Bob took the white one, even if both books are currently not visible by Bob it understands that Bob speaks about the black book. The robot will answer: "It is on the table behind the pink box".
% Such dialog ability is only possible if the robot
% holds correct assumption concerning human's knowledge as done by our
% system. Without the temporal reasonning on human actions, robot would have to ask "which book are you talking about?".

% %comment dans le texte, essaie de bien grouper chacun des
% %  exemples dans un paragraphe, si tu regardes le résultat pour le
% %  moment, c'est pas évident de voir la séparation

% Now, come back to the end of the scenario of fig
%   ~\ref{divB} where Greg has a wrong belief about the white book's
%   position (symbolized by an opaque green sphere). Seeing Greg trying
%   to have a look behind the white box, the robot can infer that he's
%   looking for the white book. Consequently, it can say proactively :
%   "The object you are looking for was taken by Bob''. Such proactive
%   dialog ability is possible with the help of our system because it
%   allows to infer human's intention from human's (wrong or lack of)
%   belief and to talk proactively to the human to correct it.
% % comment fin du second exemple, mettre la phrase de
% %  conclusion à part


% This level of human understanding allows the robot to interact in a more natural way with humans.









% TODO: chapter on intention -> put here?

\ifdefined\included
\else
\bibliographystyle{acm}
\bibliography{These}
\end{document}
\fi
